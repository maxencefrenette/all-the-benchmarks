benchmark: ARC-AGI-1
description: Accuracy on ARC-AGI-1
results:
  Human Panel: 98
  Stem Grad: 98
  Avg. Mturker: 77
  o3-preview (Low)*: 75.7
  o3 (High): 60.8
  o3-Pro (High): 59.3
  o4-mini (High): 58.7
  o3-Pro (Medium): 57
  ARChitects: 56
  o3 (Medium): 53.8
  o3-Pro (Low): 44.3
  o4-mini (Medium): 41.8
  o3 (Low): 41.5
  Gemini 2.5 Pro (Thinking 16K): 41
  Claude Sonnet 4 (Thinking 16K): 40
  Gemini 2.5 Pro (Thinking 32K): 37
  Claude Opus 4 (Thinking 16K): 35.7
  o3-mini (High): 34.5
  Gemini 2.5 Flash (Preview): 33.3
  Gemini 2.5 Flash (Preview) (Thinking 16K): 33.3
  Gemini 2.5 Flash (Preview) (Thinking 24K): 32.3
  o1 (Medium): 30.7
  Claude Opus 4 (Thinking 8K): 30.7
  Gemini 2.5 Pro (Thinking 8K): 29.5
  Claude Sonnet 4 (Thinking 8K): 29
  Claude 3.7 (16K): 28.6
  Claude Sonnet 4 (Thinking 1K): 28
  Codex Mini (Latest): 27.3
  o1 (Low): 27.2
  Claude Opus 4 (Thinking 1K): 27
  Gemini 2.5 Flash (Preview) (Thinking 8K): 25.8
  Claude Sonnet 4: 23.8
  o1-pro (Low): 23.3
  Claude Opus 4: 22.5
  o3-mini (Medium): 22.3
  o4-mini (Low): 21.3
  Deepseek R1 (05/28): 21.2
  Claude 3.7 (8K): 21.2
  o1-preview: 18
  Icecuber: 17
  Grok 3 Mini (Low): 16.5
  Gemini 2.5 Flash (Preview) (Thinking 1K): 16
  Gemini 2.5 Pro (Thinking 1K): 16
  Deepseek R1: 15.8
  o3-mini (Low): 14.5
  o1-mini: 14
  Claude 3.7: 13.6
  Claude 3.7 (1K): 11.6
  GPT-4.5: 10.3
  Magistral Medium (Thinking): 6.1
  Magistral Medium: 5.9
  GPT-4.1: 5.5
  Grok 3: 5.5
  Magistral Small: 5
  GPT-4o: 4.5
  Llama 4 Maverick: 4.4
  GPT-4.1-Mini: 3.5
  Llama 4 Scout: 0.5
  GPT-4.1-Nano: 0
cost_per_task:
  Human Panel: 17
  Stem Grad: 10
  Avg. Mturker: 3
  o3-preview (Low)*: 200
  o3 (High): 0.5002
  o3-Pro (High): 4.16
  o4-mini (High): 0.4058
  o3-Pro (Medium): 3.1766
  ARChitects: 0.2
  o3 (Medium): 0.2882
  o3-Pro (Low): 1.6382
  o4-mini (Medium): 0.15
  o3 (Low): 0.1764
  Gemini 2.5 Pro (Thinking 16K): 0.4839
  Claude Sonnet 4 (Thinking 16K): 0.3658
  Gemini 2.5 Pro (Thinking 32K): 0.5123
  Claude Opus 4 (Thinking 16K): 1.2496
  o3-mini (High): 0.3989
  Gemini 2.5 Flash (Preview): 0.0371
  Gemini 2.5 Flash (Preview) (Thinking 16K): 0.2134
  Gemini 2.5 Flash (Preview) (Thinking 24K): 0.1971
  o1 (Medium): 1.9329
  Claude Opus 4 (Thinking 8K): 0.7408
  Gemini 2.5 Pro (Thinking 8K): 0.2947
  Claude Sonnet 4 (Thinking 8K): 0.1952
  Claude 3.7 (16K): 0.33
  Claude Sonnet 4 (Thinking 1K): 0.0937
  Codex Mini (Latest): 0.1597
  o1 (Low): 1.1803
  Claude Opus 4 (Thinking 1K): 0.5021
  Gemini 2.5 Flash (Preview) (Thinking 8K): 0.1344
  Claude Sonnet 4: 0.0806
  o1-pro (Low): 10.9039
  Claude Opus 4: 0.4036
  o3-mini (Medium): 0.1907
  o4-mini (Low): 0.0406
  Deepseek R1 (05/28): 0.0464
  Claude 3.7 (8K): 0.21
  o1-preview: 1.6415
  Icecuber: 0.2
  Grok 3 Mini (Low): 0.0099
  Gemini 2.5 Flash (Preview) (Thinking 1K): 0.0356
  Gemini 2.5 Pro (Thinking 1K): 0.0573
  Deepseek R1: 0.06
  o3-mini (Low): 0.0519
  o1-mini: 0.135
  Claude 3.7: 0.058
  Claude 3.7 (1K): 0.07
  GPT-4.5: 0.29
  Magistral Medium (Thinking): 0.0989
  Magistral Medium: 0.1015
  GPT-4.1: 0.039
  Grok 3: 0.0931
  Magistral Small: 0.0399
  GPT-4o: 0.05
  Llama 4 Maverick: 0.0078
  GPT-4.1-Mini: 0.0078
  Llama 4 Scout: 0.0041
  GPT-4.1-Nano: 0.0021
model_name_mapping_file: arc-agi.yaml
