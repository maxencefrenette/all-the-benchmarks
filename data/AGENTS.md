# Data directory overview

This folder stores YAML files consumed by the application and populated by scraping scripts.

## Subdirectories

- `raw/benchmarks/` – Raw benchmark result files. Each includes `benchmark`, `description`, `website`, `github`, `score_weight`, `cost_weight`, `results`, `model_name_mapping_file` and `private_holdout`. A `cost_per_task` mapping may also be present. The scraping scripts under `scripts/` update the `results` and `cost_per_task` sections automatically.
- `mappings/` – Model name mapping files. When a scraping script writes a benchmark file, it also ensures the corresponding mapping YAML is updated with any new model names. Mapping values that associate an alias to a known model slug are maintained manually.
- `models/` – Model definitions. Each file lists the provider and a `reasoning_efforts:` mapping that associates one or more slugs with their human friendly names. Files may also specify `aliases`, `deprecated`, and `release_date`.
- `processed/benchmarks/` – Normalized benchmark outputs generated by running `uv run process_data.py` in `scripts_python/`. Do not edit these files by hand.

## How the code interacts with these files

- `lib/data-loader.ts` reads YAML from `models/`, `raw/benchmarks/`, `mappings/` and `processed/benchmarks/` to merge scores and compute per‑model statistics used throughout the site.
- `lib/benchmark-loader.ts` loads benchmark metadata and details from `raw/benchmarks/` for individual benchmark pages.
- Next.js route handlers in `app/` call these loader functions to build pages.
- Scraping utilities in `scripts/` (e.g. `scrape_livebench.ts`) download data from external leaderboards and call `saveBenchmarkResults` from `scripts/utils.ts` to update benchmark files. After scraping, run `uv run update_mappings.py` from `scripts_python/` to add any new models to the mapping files.
- Run `uv run process_data.py` from `scripts_python/` whenever benchmark YAML files change to refresh the `processed/benchmarks/` directory.

## Generation details

- Files inside `models/` are **manually generated**.
- Files inside `raw/benchmarks/` and `mappings/` are **partially automatically generated** – the scraping scripts overwrite score sections and add new model keys but preserve manual descriptions and mappings.
- Files inside `processed/benchmarks/` are **fully automatically generated** by `process_data.py` and should never be edited by hand.
