# Data directory overview

This folder stores YAML files consumed by the application and populated by scraping scripts.

## Subdirectories

- `raw/benchmarks/` – Raw benchmark result files. Each includes `benchmark`, `description`, `website`, `github`, `score_weight`, `cost_weight`, `results`, `model_name_mapping_file` and `private_holdout`. A `cost_per_task` mapping may also be present. The scraping scripts under `scripts/` update the `results` and `cost_per_task` sections automatically.
- `config/mappings/` – Model name mapping files. When a scraping script writes a benchmark file, it also ensures the corresponding mapping YAML is updated with any new model names. Review these files after running `uv run update_mappings.py` to ensure the aliases map to the correct canonical slugs.
- `config/models/` – Model definitions. Each file lists the provider and a `reasoning_efforts:` mapping that associates one or more slugs with their human friendly names. Files may also specify `aliases`, `deprecated`, and `release_date`.
- `processed/benchmarks/` – Normalized benchmark outputs generated by running `uv run process_data.py` in `scripts_python/`. These files are committed to the repository for transparency. Do not edit them by hand.

## How the code interacts with these files

- `lib/data-loader.ts` reads YAML from `config/models/`, `raw/benchmarks/`, `config/mappings/` and `processed/benchmarks/` to merge scores and compute per‑model statistics used throughout the site.
- `lib/benchmark-loader.ts` loads benchmark metadata and details from `raw/benchmarks/` for individual benchmark pages.
- Next.js route handlers in `app/` call these loader functions to build pages.
- Scraping utilities in `scripts/` (e.g. `scrape_livebench.ts`) download data from external leaderboards and call `saveBenchmarkResults` from `scripts/utils.ts` to update benchmark files. After scraping, run `uv run update_mappings.py` from `scripts_python/` to add any new models to the mapping files.
- Run `uv run process_data.py` from `scripts_python/` whenever benchmark YAML files change to refresh the `processed/benchmarks/` directory.
- Commit the updated files under `processed/benchmarks/` so the application can load the new data in production.

## Generation details

- Files inside `config/models/` are **manually generated**.
- Files inside `raw/benchmarks/` and `config/mappings/` are **partially automatically generated** – the scraping scripts overwrite score sections and add new model keys but preserve manual descriptions and mappings.
- Files inside `processed/benchmarks/` are **fully automatically generated** by `process_data.py` and should never be edited by hand.
