results:
  GLM-4.5 (FC): 70.85
  Claude-Opus-4-1-20250805 (FC): 70.36
  Claude-Sonnet-4-20250514 (FC): 70.29
  GLM-4.5-Air (FC): 67.87
  Grok-4-0709 (Prompt): 61.6
  Grok-4-0709 (FC): 61.01
  GPT-5-2025-08-07 (FC): 59.22
  o3-2025-04-16 (Prompt): 58.76
  Moonshotai-Kimi-K2-Instruct (FC): 56.07
  Moonshotai-Kimi-K2-Instruct (Prompt): 55.3
  o4-mini-2025-04-16 (Prompt): 54.81
  Qwen3-235B-A22B-Instruct-2507 (FC): 54.37
  o3-2025-04-16 (FC): 54.36
  GPT-5-mini-2025-08-07 (FC): 54.21
  Gemini-2.5-Pro (Prompt): 54.14
  Gemini-2.5-Flash (FC): 53.63
  o4-mini-2025-04-16 (FC): 53.25
  xLAM-2-70b-fc-r (FC): 52.94
  xLAM-2-32b-fc-r (FC): 52.64
  watt-tool-70B (FC): 51.25
  Gemini-2.5-Pro (FC): 50.92
  GPT-4o-2024-11-20 (FC): 50.27
  DeepSeek-R1-0528 (FC): 48.97
  Qwen3-32B (FC): 48.88
  GPT-5-nano-2025-08-07 (FC): 48.75
  Amazon-Nova-Pro-v1:0 (FC): 47.77
  Command A (FC): 47.19
  Qwen3-235B-A22B-Instruct-2507 (Prompt): 47.06
  Gemini-2.5-Flash (Prompt): 47.06
  BitAgent-Bounty-8B: 46.6
  xLAM-2-8b-fc-r (FC): 46.56
  Qwen3-14B (FC): 46.31
  Arch-Agent-32B: 45.88
  Qwen3-32B (Prompt): 45.67
  DeepSeek-V3-0324 (FC): 45.2
  QwQ-32B (Prompt): 45.11
  mistral-large-2411 (FC): 44.25
  GPT-4o-mini-2024-07-18 (FC): 43.92
  claude-3.5-haiku-20241022 (FC): 43.42
  Qwen3-8B (FC): 42.59
  ToolACE-2-8B (FC): 42.58
  Qwen3-30B-A3B-Instruct-2507 (FC): 42.41
  Qwen3-14B (Prompt): 42.4
  CoALM-405B: 41.65
  Mistral-small-2506 (FC): 40.93
  GPT-5-2025-08-07 (Prompt): 40.81
  xLAM-2-3b-fc-r (FC): 40.68
  DM-Cito-8B-v2 (Prompt): 40.08
  Mistral-Medium-2505 (FC): 39.62
  BitAgent-8B: 39.32
  watt-tool-8B (FC): 38.69
  Amazon-Nova-Lite-v1:0 (FC): 38.48
  Qwen3-8B (Prompt): 38.18
  DeepSeek-R1-0528 (Prompt): 37.38
  Llama-4-Maverick-17B-128E-Instruct-FP8 (FC): 36.37
  QwQ-32B (FC): 35.63
  Qwen3-30B-A3B-Instruct-2507 (Prompt): 35.57
  Arch-Agent-3B: 35.31
  Gemini-2.5-Flash-Lite-Preview-06-17 (FC): 35.1
  Qwen3-4B-Instruct-2507 (FC): 34.35
  xLAM-2-1b-fc-r (FC): 34.05
  Arch-Agent-1.5B: 33.52
  Mistral-Small-2506 (Prompt): 33.29
  claude-3.5-haiku-20241022 (Prompt): 32.66
  Hammer2.1-7b (FC): 32.66
  Mistral-Medium-2505: 32.65
  Amazon-Nova-Micro-v1:0 (FC): 32.18
  Llama-3.1-70B-Instruct (Prompt): 31.55
  Qwen3-4B-Instruct-2507 (Prompt): 31.28
  Llama-3.3-70B-Instruct (FC): 30.76
  Claude-Sonnet-4-20250514 (Prompt): 30.41
  Haha-7B: 29.84
  mistral-large-2411 (Prompt): 29.83
  Hammer2.1-3b (FC): 29.81
  Gemma-3-27b-it (Prompt): 29.8
  Gemma-3-12b-it (Prompt): 29.73
  GPT-4o-mini-2024-07-18 (Prompt): 29.56
  Qwen3-1.7B (FC): 28.83
  Command R7B (FC): 28.52
  Gemini-2.5-Flash-Lite-Preview-06-17 (Prompt): 28.31
  Phi-4 (Prompt): 27.81
  Claude-Opus-4-1-20250805 (Prompt): 27.74
  Qwen3-1.7B (Prompt): 27.6
  palmyra-x-004 (FC): 27.51
  GPT-4o-2024-11-20 (Prompt): 27.51
  GPT-5-mini-2025-08-07 (Prompt): 27.21
  Open-Mistral-Nemo-2407 (FC): 27.2
  Hammer2.1-1.5b (FC): 27.03
  CoALM-70B: 26.88
  Ministral-8B-Instruct-2410 (FC): 26.77
  Granite-3.1-8B-Instruct (FC): 26.35
  Falcon3-10B-Instruct (FC): 26.35
  CoALM-8B: 26.22
  Llama-4-Scout-17B-16E-Instruct (FC): 26
  Granite-3.2-8B-Instruct (FC): 25.76
  GPT-5-nano-2025-08-07 (Prompt): 25.45
  MiniCPM3-4B-FC (FC): 24.94
  Llama-3.1-8B-Instruct (Prompt): 24.62
  Falcon3-7B-Instruct (FC): 23.83
  Granite-20b-FunctionCalling (FC): 22.75
  Qwen3-0.6B (FC): 22.64
  RZN-T (Prompt): 22.22
  MiniCPM3-4B (Prompt): 21.23
  GLM-4-9b-Chat (FC): 21.19
  Bielik-11B-v2.3-Instruct (Prompt): 21.14
  Llama-3.2-3B-Instruct (FC): 20.88
  Hammer2.1-0.5b (FC): 20.84
  Gemma-3-4b-it (Prompt): 20.75
  Open-Mistral-Nemo-2407 (Prompt): 19.12
  Qwen3-0.6B (Prompt): 19.12
  Llama-3.1-70B-Instruct (FC): 17.64
  Falcon3-3B-Instruct (FC): 16.22
  Llama-3.1-8B-Instruct (FC): 15.58
  Phi-4-mini-instruct (FC): 11.05
  Llama-3.2-1B-Instruct (FC): 10.85
  Falcon3-1B-Instruct (FC): 10.13
  ThinkAgent-1B (FC): 10.09
  Gemma-3-1b-it (Prompt): 7.19
cost_per_task:
  GLM-4.5 (FC): 2.9
  Claude-Opus-4-1-20250805 (FC): 207.12
  Claude-Sonnet-4-20250514 (FC): 41.49
  GLM-4.5-Air (FC): 4.22
  Grok-4-0709 (Prompt): 333.24
  Grok-4-0709 (FC): 329.44
  GPT-5-2025-08-07 (FC): 159.16
  o3-2025-04-16 (Prompt): 235.89
  Moonshotai-Kimi-K2-Instruct (FC): 6.94
  Moonshotai-Kimi-K2-Instruct (Prompt): 6.26
  o4-mini-2025-04-16 (Prompt): 138.16
  Qwen3-235B-A22B-Instruct-2507 (FC): 12.02
  o3-2025-04-16 (FC): 136.76
  GPT-5-mini-2025-08-07 (FC): 21.14
  Gemini-2.5-Pro (Prompt): 212.36
  Gemini-2.5-Flash (FC): 26.32
  o4-mini-2025-04-16 (FC): 82.46
  xLAM-2-70b-fc-r (FC): 4.7
  xLAM-2-32b-fc-r (FC): 2.17
  watt-tool-70B (FC): 5.13
  Gemini-2.5-Pro (FC): 132.77
  GPT-4o-2024-11-20 (FC): 133.58
  DeepSeek-R1-0528 (FC): 53.04
  Qwen3-32B (FC): 11.07
  GPT-5-nano-2025-08-07 (FC): 8.99
  Amazon-Nova-Pro-v1:0 (FC): 70.11
  Command A (FC): 96.3
  Qwen3-235B-A22B-Instruct-2507 (Prompt): 10.74
  Gemini-2.5-Flash (Prompt): 30.12
  BitAgent-Bounty-8B: 3.09
  xLAM-2-8b-fc-r (FC): 1.74
  Qwen3-14B (FC): 6.92
  Arch-Agent-32B: 7.35
  Qwen3-32B (Prompt): 14.32
  DeepSeek-V3-0324 (FC): 6.11
  QwQ-32B (Prompt): 1.43
  mistral-large-2411 (FC): 123.13
  GPT-4o-mini-2024-07-18 (FC): 8.33
  claude-3.5-haiku-20241022 (FC): 10.66
  Qwen3-8B (FC): 7.2
  ToolACE-2-8B (FC): 1.89
  Qwen3-30B-A3B-Instruct-2507 (FC): 4.14
  Qwen3-14B (Prompt): 9.54
  CoALM-405B: 23.6
  Mistral-small-2506 (FC): 5.8
  GPT-5-2025-08-07 (Prompt): 388.94
  xLAM-2-3b-fc-r (FC): 1.16
  DM-Cito-8B-v2 (Prompt): 7.59
  Mistral-Medium-2505 (FC): 19.78
  BitAgent-8B: 1.69
  watt-tool-8B (FC): 1.57
  Amazon-Nova-Lite-v1:0 (FC): 5.5
  Qwen3-8B (Prompt): 10.1
  DeepSeek-R1-0528 (Prompt): 54.34
  Llama-4-Maverick-17B-128E-Instruct-FP8 (FC): 4.63
  QwQ-32B (FC): 1.11
  Qwen3-30B-A3B-Instruct-2507 (Prompt): 4.89
  Arch-Agent-3B: 1.24
  Gemini-2.5-Flash-Lite-Preview-06-17 (FC): 6.71
  Qwen3-4B-Instruct-2507 (FC): 1.21
  xLAM-2-1b-fc-r (FC): 1.31
  Arch-Agent-1.5B: 1.47
  Mistral-Small-2506 (Prompt): 7.3
  claude-3.5-haiku-20241022 (Prompt): 13.31
  Hammer2.1-7b (FC): 0.81
  Mistral-Medium-2505: 34.09
  Amazon-Nova-Micro-v1:0 (FC): 2.6
  Llama-3.1-70B-Instruct (Prompt): 5.77
  Qwen3-4B-Instruct-2507 (Prompt): 1.98
  Llama-3.3-70B-Instruct (FC): 4.66
  Claude-Sonnet-4-20250514 (Prompt): 48.57
  Haha-7B: 1.57
  mistral-large-2411 (Prompt): 214.6
  Hammer2.1-3b (FC): 0.93
  Gemma-3-27b-it (Prompt): 10.48
  Gemma-3-12b-it (Prompt): 3.19
  GPT-4o-mini-2024-07-18 (Prompt): 14.91
  Qwen3-1.7B (FC): 2.89
  Command R7B (FC): 1.5
  Gemini-2.5-Flash-Lite-Preview-06-17 (Prompt): 7.16
  Phi-4 (Prompt): 1.81
  Claude-Opus-4-1-20250805 (Prompt): 244.72
  Qwen3-1.7B (Prompt): 3.52
  palmyra-x-004 (FC): 180.68
  GPT-4o-2024-11-20 (Prompt): 290.58
  GPT-5-mini-2025-08-07 (Prompt): 87.12
  Open-Mistral-Nemo-2407 (FC): 7.89
  Hammer2.1-1.5b (FC): 0.93
  CoALM-70B: 4.51
  Ministral-8B-Instruct-2410 (FC): 4.91
  Granite-3.1-8B-Instruct (FC): 1.95
  Falcon3-10B-Instruct (FC): 2.41
  CoALM-8B: 1.81
  Llama-4-Scout-17B-16E-Instruct (FC): 5
  Granite-3.2-8B-Instruct (FC): 1.44
  GPT-5-nano-2025-08-07 (Prompt): 21.5
  MiniCPM3-4B-FC (FC): 1.43
  Llama-3.1-8B-Instruct (Prompt): 1.62
  Falcon3-7B-Instruct (FC): 3.47
  Granite-20b-FunctionCalling (FC): 1.83
  Qwen3-0.6B (FC): 2.89
  RZN-T (Prompt): 3.56
  MiniCPM3-4B (Prompt): 2.82
  GLM-4-9b-Chat (FC): 1.19
  Bielik-11B-v2.3-Instruct (Prompt): 2.11
  Llama-3.2-3B-Instruct (FC): 1.24
  Hammer2.1-0.5b (FC): 0.98
  Gemma-3-4b-it (Prompt): 1.56
  Open-Mistral-Nemo-2407 (Prompt): 13.35
  Qwen3-0.6B (Prompt): 5.18
  Llama-3.1-70B-Instruct (FC): 3.8
  Falcon3-3B-Instruct (FC): 2.51
  Llama-3.1-8B-Instruct (FC): 1.03
  Phi-4-mini-instruct (FC): 20.41
  Llama-3.2-1B-Instruct (FC): 0.57
  Falcon3-1B-Instruct (FC): 0.35
  ThinkAgent-1B (FC): 2.87
  Gemma-3-1b-it (Prompt): 1.32
benchmark: Berkeley Function Call Leaderboard (BFCL)
description: Overall accuracy on BFCL (function calling)
website: https://gorilla.cs.berkeley.edu/leaderboard.html
github: null
score_weight: 1
cost_weight: 1
model_name_mapping_file: gorilla-bfcl.yaml
private_holdout: true
