benchmark: MathArena Project Euler
description: Accuracy on MathArena Project Euler competition
website: https://matharena.ai/
github: https://github.com/eth-sri/matharena
score_weight: 1
cost_weight: 1
model_name_mapping_file: matharena.yaml
private_holdout: false
results:
  Claude-Sonnet-4.0: 3.8461538461538463
  Grok 4: 25
  gemini-2.5-pro: 9.615384615384615
  o4-mini (high): 36.53846153846154
cost_per_task:
  Claude-Sonnet-4.0: 7.809861
  Grok 4: 70.00760100000001
  gemini-2.5-pro: 27.7332025
  o4-mini (high): 27.7072631
