benchmark: MathArena Project Euler
description: Accuracy on MathArena Project Euler competition
website: https://matharena.ai/
github: https://github.com/eth-sri/matharena
score_weight: 1
cost_weight: 1
results:
  Claude-Sonnet-4.0: 3.3333333333333335
  Grok 4: 28.333333333333336
  gemini-2.5-pro: 8.333333333333334
  o4-mini (high): 38.333333333333336
model_name_mapping_file: matharena.yaml
private_holdout: false
cost_per_task:
  Claude-Sonnet-4.0: 10.192359
  Grok 4: 77.05681200000001
  gemini-2.5-pro: 31.419613750000003
  o4-mini (high): 29.649027099999998
