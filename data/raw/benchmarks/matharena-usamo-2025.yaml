benchmark: MathArena USAMO 2025
description: Accuracy on MathArena USAMO 2025 competition
website: https://matharena.ai/
github: https://github.com/eth-sri/matharena
score_weight: 1
cost_weight: 1
results:
  Claude-3.7-Sonnet (Think): 3.645833333333333
  DeepSeek-R1: 4.761904761904762
  DeepSeek-R1-0528: 30.05952380952381
  Grok 3 (Think): 4.761904761904762
  QwQ-32B: 2.9761904761904763
  gemini-2.0-flash-thinking: 4.166666666666667
  gemini-2.5-pro: 24.404761904761905
  o1-pro (high): 2.8273809523809526
  o3 (high): 21.726190476190474
  o3-mini (high): 2.0833333333333335
  o4-mini (high): 19.047619047619047
model_name_mapping_file: matharena.yaml
private_holdout: false
cost_per_task:
  Claude-3.7-Sonnet (Think): 9.032831999999999
  DeepSeek-R1: 2.029706
  DeepSeek-R1-0528: 0.9123225400000001
  QwQ-32B: 0.41695199999999993
  gemini-2.5-pro: 6.232095
  o1-pro (high): 203.4444856
  o3 (high): 24.169919999999998
  o3-mini (high): 1.1149292
  o4-mini (high): 2.207106
