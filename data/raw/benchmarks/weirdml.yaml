benchmark: WeirdML
description: Average accuracy across WeirdML tasks
website: https://htihle.github.io/weirdml.html
github: null
score_weight: 1
cost_weight: 1
results:
  o3-pro-2025-06-10 (high): 53.95
  gemini-2.5-pro (thinking 16k): 50.3
  o3-2025-04-16 (high): 49.76
  o4-mini-2025-04-16 (high): 49.17
  claude-4-sonnet-20250522 (thinking 16k): 45.28
  claude-4-sonnet-20250522 (no thinking): 43
  grok-4-07-09: 42.55
  claude-4-opus-20250522 (thinking 16k): 42.12
  deepseek-r1-0528: 40.88
  glm-4.5 (thinking): 40.76
  qwen3-coder: 40.71
  claude-3.6-sonnet: 39.78
  grok-3-mini (high): 39.58
  qwen3-235b-a22b-thinking-2507: 38.88
  gemini-2.5-flash (thinking 16k): 38.73
  kimi-k2: 38.58
  gpt-4.1-2025-04-14: 37.88
  qwen3-235b-a22b-07-25: 37.72
  gpt-4.5-preview: 37.65
  gpt-4.1-mini-2025-04-14: 37.25
  grok-3: 36.44
  qwen3-235b-a22b (thinking): 36.25
  deepseek-v3-0324: 35.09
  gemini-2.5-flash-lite-preview-06-17 (thinking 16k): 33.89
  qwen3-30b-a3b (thinking): 29.74
  llama-4-maverick: 23.62
  grok-2-1212: 22.04
  gpt-4.1-nano-2025-04-14: 19.04
model_name_mapping_file: weirdml.yaml
private_holdout: false
cost_per_task:
  o3-pro-2025-06-10 (high): 5.228346341463415
  gemini-2.5-pro (thinking 16k): 0.8231632972631578
  o3-2025-04-16 (high): 0.46371908571428566
  o4-mini-2025-04-16 (high): 0.38685772040816324
  claude-4-sonnet-20250522 (thinking 16k): 0.667855163265306
  claude-4-sonnet-20250522 (no thinking): 0.6080159361702129
  grok-4-07-09: 0.9339984292035399
  claude-4-opus-20250522 (thinking 16k): 3.3979484210526314
  deepseek-r1-0528: 0.1420080642105263
  glm-4.5 (thinking): 0.14728206857142853
  qwen3-coder: 0.14427242105263158
  claude-3.6-sonnet: 0.4931495368421052
  grok-3-mini (high): 0.034985129999999996
  qwen3-235b-a22b-thinking-2507: 0.039607944000000006
  gemini-2.5-flash (thinking 16k): 0.21184530736842114
  kimi-k2: 0.06815525026666666
  gpt-4.1-2025-04-14: 0.19657672340425536
  qwen3-235b-a22b-07-25: 0.02639886902654867
  gpt-4.5-preview: 3.1601368421052634
  gpt-4.1-mini-2025-04-14: 0.03296698105263158
  grok-3: 0.5004218315789474
  qwen3-235b-a22b (thinking): 0.04171634687022901
  deepseek-v3-0324: 0.027581992
  gemini-2.5-flash-lite-preview-06-17 (thinking 16k): 0.060986399999999996
  qwen3-30b-a3b (thinking): 0.016747094143646408
  llama-4-maverick: 0.012274919999999998
  grok-2-1212: 0.22587717894736845
  gpt-4.1-nano-2025-04-14: 0.005178074782608696
