benchmark: MathArena HMMT February 2025
description: Accuracy on MathArena HMMT February 2025 competition
website: https://matharena.ai/
github: https://github.com/eth-sri/matharena
score_weight: 0
cost_weight: 0
model_name_mapping_file: matharena.yaml
private_holdout: false
results:
  Claude-3.5-Sonnet: 1.6666666666666667
  Claude-3.7-Sonnet (Think): 31.666666666666664
  Claude-Opus-4.0 (Think): 58.33333333333335
  DeepSeek-R1: 41.66666666666667
  DeepSeek-R1-0528: 76.66666666666666
  DeepSeek-R1-Distill-1.5B: 11.666666666666668
  DeepSeek-R1-Distill-14B: 31.666666666666664
  DeepSeek-R1-Distill-32B: 33.33333333333333
  DeepSeek-R1-Distill-70B: 33.333333333333336
  DeepSeek-V3: 13.333333333333332
  DeepSeek-V3-03-24: 29.166666666666664
  GLM 4.5: 77.5
  GLM 4.5 Air: 69.16666666666667
  GPT OSS 120B (high): 90
  GPT OSS 20B (high): 74.99999999999999
  Grok 3 Mini (high): 74.16666666666667
  Grok 3 Mini (low): 50.83333333333334
  Grok 4: 92.49999999999997
  QwQ-32B: 47.50000000000001
  QwQ-32B-Preview: 18.333333333333336
  Qwen3-235B-A22B: 62.50000000000001
  Qwen3-30B-A3B: 50.83333333333334
  gemini-2.0-flash: 13.333333333333334
  gemini-2.0-flash-thinking: 35.83333333333333
  gemini-2.0-pro: 7.5
  gemini-2.5-flash (think): 64.16666666666667
  gemini-2.5-pro: 82.49999999999999
  gemini-2.5-pro-05-06: 80.83333333333333
  gpt-4o: 5.833333333333334
  o1 (medium): 48.33333333333334
  o3 (high): 77.5
  o3-mini (high): 67.50000000000003
  o3-mini (low): 28.33333333333333
  o3-mini (medium): 53.33333333333334
  o4-mini (high): 82.5
  o4-mini (low): 47.50000000000001
  o4-mini (medium): 66.66666666666667
cost_per_task:
  Claude-3.5-Sonnet: 1.0012709999999998
  Claude-3.7-Sonnet (Think): 46.684985999999995
  Claude-Opus-4.0 (Think): 152.635605
  DeepSeek-R1: 3.36028166
  DeepSeek-R1-0528: 6.674237060000001
  DeepSeek-R1-Distill-1.5B: 0.50192892
  DeepSeek-R1-Distill-14B: 0.2823537
  DeepSeek-R1-Distill-32B: 0.5646036
  DeepSeek-R1-Distill-70B: 0.8596183999999999
  DeepSeek-V3: 0.38610999999999995
  DeepSeek-V3-03-24: 0.6237932
  GLM 4.5: 6.724627800000001
  GLM 4.5 Air: 3.6620837000000006
  GPT OSS 120B (high): 1.9661693999999998
  GPT OSS 20B (high): 3.9415482000000006
  Grok 3 Mini (high): 1.2643894
  Grok 3 Mini (low): 0.40256090000000005
  Grok 4: 28.337499
  QwQ-32B: 2.3463983999999996
  QwQ-32B-Preview: 1.3930056000000002
  Qwen3-235B-A22B: 1.0902295999999998
  Qwen3-30B-A3B: 0.6690028
  gemini-2.0-flash: 0.16042400000000004
  gemini-2.0-pro: 1.3687496
  gemini-2.5-flash (think): 11.412851
  gemini-2.5-pro: 15.46807
  gpt-4o: 0.9604900000000001
  o1 (medium): 107.0409
  o3 (high): 14.210788
  o3-mini (high): 9.34109
  o3-mini (low): 1.4215740000000003
  o3-mini (medium): 4.030259200000001
  o4-mini (high): 9.37981
  o4-mini (low): 1.4228588000000002
  o4-mini (medium): 3.8701256000000006
