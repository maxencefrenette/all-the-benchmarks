benchmark: Humanity's Last Exam
description: Score on Scale's Humanity's Last Exam
website: https://artificialanalysis.ai/evaluations/humanitys-last-exam
github: null
score_weight: 1
cost_weight: 1
results:
  GPT-5 (high): 26
  Grok 4: 23
  GPT-5 (medium): 23
  Gemini 2.5 Pro: 21
  o3: 20
  GPT-5 mini (high): 19
  gpt-oss-120B (high): 18
  GPT-5 (low): 18
  o4-mini (high): 17
  Gemini 2.5 Pro (Mar '25): 17
  Gemini 2.5 Pro (May' 25): 15
  Qwen3 235B 2507 (Reasoning): 15
  DeepSeek R1 0528: 14
  GPT-5 mini (medium): 14
  DeepSeek V3.1 (Reasoning): 13
  o3-mini (high): 12
  GLM-4.5: 12
  Claude 4.1 Opus Thinking: 11
  Claude 4 Opus Thinking: 11
  Qwen3 235B (Reasoning): 11
  Gemini 2.5 Flash (April '25) (Reasoning): 11
  Gemini 2.5 Flash (Reasoning): 11
  Grok 3 mini Reasoning (high): 11
  Qwen3 235B 2507 (Non-reasoning): 10
  EXAONE 4.0 32B (Reasoning): 10
  Claude 3.7 Sonnet Thinking: 10
  Qwen3 30B 2507 (Reasoning): 9
  Claude 4 Sonnet Thinking: 9
  Magistral Medium: 9
  DeepSeek R1 (Jan '25): 9
  o3-mini: 8
  gpt-oss-20B (high): 8
  Qwen3 32B (Reasoning): 8
  GPT-5 nano (high): 8
  MiniMax M1 80k: 8
  QwQ-32B: 8
  Llama Nemotron Ultra Reasoning: 8
  Hermes 4 - Llama-3.1 70B (Reasoning): 7
  Sonar Pro: 7
  o1: 7
  GPT-5 nano (medium): 7
  Grok Code Fast 1: 7
  MiniMax M1 40k: 7
  Sonar: 7
  Magistral Small: 7
  Gemini 2.0 Flash Thinking exp. (Jan '25): 7
  Solar Pro 2 (Reasoning): 7
  Kimi K2: 7
  Llama Nemotron Super 49B v1.5 (Reasoning): 6
  GLM-4.5-Air: 6
  Qwen3 30B 2507 (Non-reasoning): 6
  Gemini 2.0 Pro Experimental: 6
  DBRX: 6
  Qwen3 30B (Reasoning): 6
  Llama 3.3 Nemotron Super 49B Reasoning: 6
  Gemini 2.5 Flash-Lite (Reasoning): 6
  DeepSeek V3.1 (Non-reasoning): 6
  DeepSeek R1 Distill Llama 70B: 6
  Claude 4 Opus: 5
  GLM-4.5V (Reasoning): 5
  Qwen3 4B 2507 (Reasoning): 5
  Exaone 4.0 1.2B (Reasoning): 5
  Exaone 4.0 1.2B: 5
  Llama 2 Chat 7B: 5
  LFM2 1.2B: 5
  Solar Pro 2  (Reasoning): 5
  Qwen3 0.6B (Reasoning): 5
  DeepSeek R1 0528 Qwen3 8B: 5
  Ministral 3B: 5
  DeepSeek R1 Distill Qwen 32B: 5
  GPT-5 (minimal): 5
  Codestral-Mamba: 5
  Llama 3.2 1B: 5
  Gemini 2.0 Flash: 5
  Pixtral 12B: 5
  DeepSeek Coder V2 Lite: 5
  Llama 3.2 11B (Vision): 5
  Gemma 3 1B: 5
  Gemma 3 4B: 5
  Llama 3.2 3B: 5
  DeepSeek V3 0324 (Mar '25): 5
  Jamba Instruct: 5
  Qwen3 1.7B: 5
  Qwen3 0.6B: 5
  Gemini 2.5 Flash: 5
  Grok 3: 5
  Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning): 5
  Reka Flash 3: 5
  Aya Expanse 8B: 5
  Llama 3.1 8B: 5
  Llama 3 8B: 5
  Codestral (May '24): 5
  Command-R: 5
  Jamba 1.5 Mini: 5
  Qwen3 4B (Reasoning): 5
  GPT-5 mini (minimal): 5
  GPT-4o (March 2025): 5
  Llama 2 Chat 70B: 5
  Gemini 2.5 Flash (April '25): 5
  Command-R+: 5
  Llama 3.2 90B (Vision): 4
  Ministral 8B: 4
  EXAONE 4.0 32B: 4
  o1-mini: 4
  Gemini 1.5 Pro (Sep): 4
  Gemma 3n E4B (May '25): 4
  LFM 40B: 4
  Llama 4 Maverick: 4
  Gemma 3 12B: 4
  Claude 3.7 Sonnet: 4
  Mistral Small 3.1: 4
  OpenChat 3.5: 4
  Command-R (Mar '24): 4
  Qwen2.5 Coder 7B: 4
  Qwen3 1.7B (Reasoning): 4
  QwQ 32B-Preview: 4
  Gemma 3 27B: 4
  Nova Premier: 4
  Llama 2 Chat 13B: 4
  Gemini 2.0 Flash (exp): 4
  Grok Beta: 4
  Nova Micro: 4
  Qwen3 235B: 4
  GPT-4.1: 4
  GPT-4.1 mini: 4
  Llama 3.1 Nemotron 70B: 4
  NVIDIA Nemotron Nano 9B V2 (Reasoning): 4
  Command A: 4
  Llama 3.1 70B: 4
  Gemini 1.0 Pro: 4
  Nova Lite: 4
  Jamba 1.6 Mini: 4
  Qwen3 30B: 4
  Codestral (Jan '25): 4
  Aya Expanse 32B: 4
  Jamba 1.7 Mini: 4
  Gemini 1.5 Flash-8B: 4
  Mixtral 8x7B: 4
  Phi-3 Medium 14B: 4
  Command-R+ (Apr '24): 4
  Qwen2.5 Max: 4
  Gemma 3n E4B: 4
  Mistral Medium 3.1: 4
  Phi-4 Multimodal: 4
  Qwen3 Coder 480B: 4
  Llama 3 70B: 4
  Gemini 2.0 Flash-Lite (Preview): 4
  Mistral Small (Feb '24): 4
  Mistral NeMo: 4
  DeepSeek R1 Distill Qwen 14B: 4
  Phi-3 Mini: 4
  Llama 4 Scout: 4
  Mistral Medium 3: 4
  Mistral Small 3.2: 4
  Llama Nemotron Super 49B v1.5: 4
  DeepHermes 3 - Llama-3.1 8B: 4
  Mistral Small (Sep '24): 4
  Mistral 7B: 4
  Qwen3 14B (Reasoning): 4
  Qwen3 32B: 4
  Llama 3.1 405B: 4
  MiniMax-Text-01: 4
  Granite 3.3 8B: 4
  Hermes 4 405B: 4
  Gemini 1.5 Flash (May): 4
  Claude 2.1: 4
  DeepSeek R1 Distill Llama 8B: 4
  Phi-4 Mini: 4
  Qwen2.5 72B: 4
  Qwen2.5 Turbo: 4
  Qwen3 8B (Reasoning): 4
  Qwen3 14B: 4
  GPT-5 nano (minimal): 4
  Phi-4: 4
  Mistral Small 3: 4
  Mixtral 8x22B: 4
  Mistral Saba: 4
  Hermes 3 - Llama-3.1 70B: 4
  GPT-4o mini: 4
  Llama 3.3 70B: 4
  Gemma 3n E2B: 4
  Claude 4 Sonnet: 4
  NVIDIA Nemotron Nano 9B V2: 4
  Qwen3 Coder 30B: 4
  Mistral Large 2 (Nov '24): 4
  Devstral Small (May '25): 4
  Jamba 1.6 Large: 4
  Jamba 1.5 Large: 4
  GPT-4.1 nano: 3
  DeepHermes 3 - Mistral 24B: 3
  Gemma 2 9B: 3
  Gemini 1.5 Pro (May): 3
  Claude 3.5 Sonnet (Oct): 3
  Devstral Medium: 3
  Solar Pro 2: 3
  Jamba 1.7 Large: 3
  Claude 3 Sonnet: 3
  Claude Instant: 3
  Grok 2: 3
  Qwen2.5 Coder 32B: 3
  Qwen2.5 Instruct 32B: 3
  GPT-4o (ChatGPT): 3
  Gemini 2.5 Flash-Lite: 3
  Devstral Small: 3
  Gemma 2 27B: 3
  Claude 3.5 Sonnet (June): 3
  Qwen2 72B: 3
  Qwen3 4B: 3
  Hermes 4 70B: 3
  GLM-4.5V: 3
  Gemini 2.0 Flash-Lite (Feb '25): 3
  Pixtral Large: 3
  DeepSeek V3 (Dec '24): 3
  Llama 3.3 Nemotron Super 49B v1: 3
  Gemini 1.5 Flash (Sep): 3
  Claude 3.5 Haiku: 3
  Tulu3 405B: 3
  Mistral Large (Feb '24): 3
  Mistral Medium: 3
  Nova Pro: 3
  GPT-4 Turbo: 3
  GPT-4o (Nov '24): 3
  DeepSeek R1 Distill Qwen 1.5B: 3
  Mistral Large 2 (Jul '24): 3
  Yi-Large: 3
  Claude 3 Opus: 3
  GPT-4o (Aug '24): 2
  GPT-4o (May '24): 2
  Qwen3 8B: 2
model_name_mapping_file: artificial-analysis.yaml
private_holdout: false
cost_per_task:
  o1: 1048
  Grok 4: 696
  Claude 4.1 Opus Thinking: 649
  Claude 3.7 Sonnet Thinking: 557
  Claude 4 Opus Thinking: 546
  GPT-5 (high): 349
  Qwen3 235B 2507 (Reasoning): 334
  Gemini 2.5 Pro: 315
  Gemini 2.5 Pro (Mar '25): 309
  Gemini 2.5 Pro (May' 25): 215
  Qwen3 235B (Reasoning): 196
  Claude 4 Sonnet Thinking: 194
  GPT-5 (medium): 187
  Magistral Medium: 171
  o3: 165
  o4-mini (high): 153
  Qwen3 32B (Reasoning): 142
  o3-mini (high): 130
  Claude 4 Opus: 110
  GLM-4.5: 94
  Claude 3 Opus: 91
  Gemini 2.5 Flash (Reasoning): 78
  MiniMax M1 40k: 73
  DeepSeek R1 0528: 72
  GPT-5 (low): 66
  DeepSeek R1 (Jan '25): 66
  Qwen3 30B 2507 (Reasoning): 65
  Qwen3 14B (Reasoning): 63
  GPT-5 mini (high): 63
  GLM-4.5-Air: 54
  Grok Code Fast 1: 51
  Magistral Small: 51
  o1-mini: 49
  DeepSeek V3.1 (Reasoning): 49
  Qwen3 30B (Reasoning): 45
  o3-mini: 45
  Grok 3: 43
  Qwen3 8B (Reasoning): 39
  GPT-4 Turbo: 39
  GPT-4o (March 2025): 37
  Llama Nemotron Ultra Reasoning: 35
  EXAONE 4.0 32B (Reasoning): 33
  Qwen3 1.7B (Reasoning): 32
  Qwen3 235B 2507 (Non-reasoning): 31
  gpt-oss-120B (high): 29
  GLM-4.5V (Reasoning): 28
  Reka Flash 3: 27
  QwQ-32B: 26
  Claude 4 Sonnet: 24
  GPT-4o (May '24): 22
  Qwen3 4B (Reasoning): 21
  Claude 2.1: 21
  Nova Premier: 20
  GPT-4o (ChatGPT): 20
  Claude 3.7 Sonnet: 20
  GPT-5 nano (high): 19
  GPT-4.1: 19
  GPT-5 mini (medium): 18
  Claude 3.5 Sonnet (June): 18
  Grok 3 mini Reasoning (high): 18
  Sonar Pro: 18
  Command-R+ (Apr '24): 17
  GPT-4o (Nov '24): 17
  Command A: 17
  Qwen3 Coder 480B: 16
  Gemini 2.5 Flash: 16
  Claude 3.5 Sonnet (Oct): 15
  Gemini 2.5 Flash-Lite (Reasoning): 15
  Qwen2.5 Max: 14
  Claude 3 Sonnet: 14
  GPT-4o (Aug '24): 13
  DeepSeek R1 Distill Llama 70B: 12
  Hermes 4 - Llama-3.1 70B (Reasoning): 12
  Qwen3 0.6B (Reasoning): 12
  Kimi K2: 12
  Command-R+: 12
  Mistral Large (Feb '24): 11
  Jamba 1.5 Large: 11
  GPT-5 (minimal): 10
  Mistral Medium 3.1: 10
  DeepSeek R1 Distill Qwen 14B: 9
  Mistral Medium: 9
  QwQ 32B-Preview: 9
  Mistral Large 2 (Nov '24): 9
  Pixtral Large: 9
  Solar Pro 2 (Reasoning): 8
  Llama 3.1 405B: 8
  Jamba 1.6 Large: 8
  Gemini 2.5 Flash-Lite: 8
  Mixtral 8x22B: 8
  GPT-5 nano (medium): 7
  Qwen3 30B 2507 (Non-reasoning): 7
  Mistral Large 2 (Jul '24): 6
  Jamba 1.7 Large: 6
  Qwen3 Coder 30B: 5
  gpt-oss-20B (high): 5
  Qwen3 32B: 5
  Qwen3 235B: 4
  GPT-4.1 mini: 4
  Qwen3 8B: 4
  DeepSeek V3.1 (Non-reasoning): 4
  Claude 3.5 Haiku: 4
  Mistral Medium 3: 4
  Nova Pro: 3
  DeepSeek V3 0324 (Mar '25): 3
  EXAONE 4.0 32B: 3
  MiniMax M1 80k: 3
  DeepSeek R1 Distill Qwen 32B: 2
  Llama 3.1 70B: 2
  DeepSeek R1 0528 Qwen3 8B: 2
  Mistral Small (Feb '24): 2
  Hermes 4 405B: 2
  Sonar: 2
  Qwen3 14B: 2
  Llama 4 Maverick: 2
  Llama 3.2 90B (Vision): 2
  Qwen3 4B: 2
  Gemini 1.0 Pro: 2
  GLM-4.5V: 2
  Aya Expanse 32B: 2
  GPT-5 mini (minimal): 2
  Aya Expanse 8B: 2
  MiniMax-Text-01: 2
  Llama 3.3 70B: 2
  Devstral Medium: 2
  DeepSeek V3 (Dec '24): 2
  Llama 4 Scout: 2
  Command-R (Mar '24): 2
  Mixtral 8x7B: 1
  Claude Instant: 1
  Gemini 2.0 Flash: 1
  Phi-3 Medium 14B: 1
  Codestral (Jan '25): 1
  Gemma 2 27B: 1
  Llama 3 70B: 1
  Qwen3 1.7B: 1
  Phi-4: 1
  GPT-4o mini: 1
  Qwen3 30B: 1
  Mistral Saba: 1
  Gemma 3 12B: 1
  Gemini 2.0 Flash-Lite (Preview): 1
  Phi-3 Mini: 1
  Llama 3.2 11B (Vision): 1
  Llama 3.1 Nemotron 70B: 1
  Codestral (May '24): 1
  Gemini 2.0 Flash-Lite (Feb '25): 1
  GPT-4.1 nano: 1
  Mistral 7B: 1
  Mistral Small (Sep '24): 1
  Command-R: 1
  Hermes 4 70B: 1
  Jamba 1.6 Mini: 1
  Jamba 1.5 Mini: 0
  Mistral Small 3: 0
  Jamba 1.7 Mini: 0
  Qwen2.5 Instruct 32B: 0
  Qwen2.5 Coder 32B: 0
  GPT-5 nano (minimal): 0
  Mistral Small 3.1: 0
  Mistral Small 3.2: 0
  Granite 3.3 8B: 0
  Hermes 3 - Llama-3.1 70B: 0
  Qwen2.5 Turbo: 0
  DeepSeek R1 Distill Llama 8B: 0
  Nova Lite: 0
  Llama 2 Chat 7B: 0
  Gemma 2 9B: 0
  Pixtral 12B: 0
  Llama 3.1 8B: 0
  Devstral Small (May '25): 0
  Devstral Small: 0
  Qwen3 0.6B: 0
  Llama 3.2 1B: 0
  Mistral NeMo: 0
  Nova Micro: 0
  Ministral 8B: 0
  Llama 3.2 3B: 0
  Gemma 3n E4B: 0
  Llama 3 8B: 0
  Gemma 3 4B: 0
  Ministral 3B: 0
