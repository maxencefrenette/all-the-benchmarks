benchmark: MathArena Overall
description: Average accuracy across MathArena competitions
website: https://matharena.ai/
github: https://github.com/eth-sri/matharena
score_weight: 1
cost_weight: 1
results:
  Claude-3.5-Sonnet: 2.5
  Claude-3.7-Sonnet (Think): 50.81761006289308
  Claude-Opus-4.0 (Think): 69.72222222222224
  DeepSeek-R1: 64.87028301886792
  DeepSeek-R1-0528: 82.14544025157232
  DeepSeek-R1-Distill-1.5B: 15.833333333333334
  DeepSeek-R1-Distill-14B: 50.97091194968553
  DeepSeek-R1-Distill-32B: 55.51100628930817
  DeepSeek-R1-Distill-70B: 53.96226415094338
  DeepSeek-V3: 19.166666666666664
  DeepSeek-V3-03-24: 39.58333333333333
  DeepSeek-v3.1 (Think): 86.37578616352201
  GLM 4.5: 83.3317610062893
  GLM 4.5 Air: 78.09669811320754
  GPT OSS 120B (high): 88.91116352201257
  GPT OSS 20B (high): 80.65408805031447
  GPT-5 (high): 91.39622641509433
  GPT-5-mini (high): 87.6941823899371
  GPT-5-nano (high): 79.5424528301887
  Grok 3 Mini (high): 77.17138364779873
  Grok 3 Mini (low): 56.4441823899371
  Grok 4: 89.46147798742138
  QwQ-32B: 56.66666666666667
  QwQ-32B-Preview: 25.833333333333332
  Qwen3-235B-A22B: 76.72169811320754
  Qwen3-30B-A3B: 66.56446540880503
  gemini-2.0-flash: 20.416666666666664
  gemini-2.0-flash-thinking: 44.58333333333334
  gemini-2.0-pro: 17.5
  gemini-2.5-flash (think): 68.88600628930817
  gemini-2.5-pro: 80.60613207547168
  gemini-2.5-pro-05-06: 84.44444444444444
  gpt-4o: 8.75
  o1 (medium): 65
  o3 (high): 85.79716981132076
  o3-mini (high): 77.08333333333334
  o3-mini (low): 38.33333333333333
  o3-mini (medium): 65
  o4-mini (high): 86.77751572327043
  o4-mini (low): 58.02358490566037
  o4-mini (medium): 75.0683962264151
model_name_mapping_file: matharena.yaml
private_holdout: false
cost_per_task:
  Claude-3.5-Sonnet: 1.0443075
  Claude-3.7-Sonnet (Think): 50.847844499999994
  Claude-Opus-4.0 (Think): 137.05744
  DeepSeek-R1: 3.3687044100000003
  DeepSeek-R1-0528: 7.170494036000001
  DeepSeek-R1-Distill-1.5B: 0.43805169
  DeepSeek-R1-Distill-14B: 0.42828108750000005
  DeepSeek-R1-Distill-32B: 0.8314658249999998
  DeepSeek-R1-Distill-70B: 0.89434635
  DeepSeek-V3: 0.39429375
  DeepSeek-V3-03-24: 0.54238524
  DeepSeek-v3.1 (Think): 5.338719712000001
  GLM 4.5: 7.4631447600000005
  GLM 4.5 Air: 3.9582728000000005
  GPT OSS 120B (high): 0.9016432448
  GPT OSS 20B (high): 0.9260223559999998
  GPT-5 (high): 20.47565775
  GPT-5-mini (high): 4.517018899999999
  GPT-5-nano (high): 1.6962911900000006
  Grok 3 Mini (high): 1.475704
  Grok 3 Mini (low): 0.4612881
  Grok 4: 32.040753
  QwQ-32B: 2.3519885999999994
  QwQ-32B-Preview: 1.2958284000000002
  Qwen3-235B-A22B: 1.1780326499999998
  Qwen3-30B-A3B: 0.6983277500000001
  gemini-2.0-flash: 0.14800000000000002
  gemini-2.0-flash-thinking: 100000
  gemini-2.0-pro: 1.6113977
  gemini-2.5-flash (think): 11.6963374
  gemini-2.5-pro: 23.943451
  gemini-2.5-pro-05-06: 100000
  gpt-4o: 1.0293450000000002
  o1 (medium): 96.24245250000001
  o3 (high): 13.349931600000001
  o3-mini (high): 7.6921834
  o3-mini (low): 1.3484614000000004
  o3-mini (medium): 3.6614754000000005
  o4-mini (high): 7.8953039
  o4-mini (low): 1.516834
  o4-mini (medium): 3.805219000000001
