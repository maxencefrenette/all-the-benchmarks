benchmark: WeirdML
description: Average accuracy across WeirdML tasks
score_weight: 1
cost_weight: 1
results:
  o3-pro-2025-06-10 (high): 53.95
  gemini-2.5-pro (thinking 16k): 50.3
  o3-2025-04-16 (high): 49.76
  o4-mini-2025-04-16 (high): 49.17
  claude-4-sonnet-20250522 (thinking 16k): 45.28
  claude-4-sonnet-20250522 (no thinking): 43
  claude-4-opus-20250522 (thinking 16k): 42.12
  deepseek-r1-0528: 40.88
  claude-3.6-sonnet: 39.78
  grok-3-mini (high): 39.58
  gemini-2.5-flash (thinking 16k): 38.73
  gpt-4.1-2025-04-14: 37.88
  gpt-4.5-preview: 37.65
  gpt-4.1-mini-2025-04-14: 37.25
  grok-3: 36.44
  qwen3-235b-a22b (thinking): 35.88
  deepseek-v3-0324: 35.09
  gemini-2.5-flash-lite-preview-06-17 (thinking 16k): 33.89
  qwen3-30b-a3b (thinking): 29.74
  llama-4-maverick: 23.62
  gpt-4.1-nano-2025-04-14: 19.04
model_name_mapping_file: weirdml.yaml
private_holdout: false
cost_per_task:
  o3-pro-2025-06-10 (high): 5.228346341463415
  gemini-2.5-pro (thinking 16k): 0.8231632972631578
  o3-2025-04-16 (high): 0.4637190857142856
  o4-mini-2025-04-16 (high): 0.38685772040816335
  claude-4-sonnet-20250522 (thinking 16k): 0.667855163265306
  claude-4-sonnet-20250522 (no thinking): 0.6080159361702129
  claude-4-opus-20250522 (thinking 16k): 3.3979484210526314
  deepseek-r1-0528: 0.1420080642105263
  claude-3.6-sonnet: 0.4931495368421054
  grok-3-mini (high): 0.034985129999999996
  gemini-2.5-flash (thinking 16k): 0.21184530736842108
  gpt-4.1-2025-04-14: 0.19657672340425536
  gpt-4.5-preview: 3.1601368421052634
  gpt-4.1-mini-2025-04-14: 0.03296698105263158
  grok-3: 0.5004218315789474
  qwen3-235b-a22b (thinking): 0.041392278230088494
  deepseek-v3-0324: 0.027581991999999996
  gemini-2.5-flash-lite-preview-06-17 (thinking 16k): 0.060986399999999996
  qwen3-30b-a3b (thinking): 0.016747094143646408
  llama-4-maverick: 0.012274920000000002
  gpt-4.1-nano-2025-04-14: 0.005178074782608696
